{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import json\n",
    "import w3lib.html\n",
    "\n",
    "from lxml import html\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        self.ua = UserAgent()\n",
    "        self.headers = req.utils.default_headers()\n",
    "    \n",
    "    def index_archive(self):\n",
    "        # Create a list of all monthly sitemap urls\n",
    "        date_input_format = '%m-%Y'\n",
    "        date_nrc_format = '%Y-%m'\n",
    "\n",
    "        # start_date = '10-1970'\n",
    "        # test start date\n",
    "        start_date = '1-2021'\n",
    "        end_date = '1-2022'\n",
    "        #end_date = datetime.today().strftime(date_input_format)\n",
    "\n",
    "        def diff_month(d1, d2):\n",
    "            return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "        start = datetime.strptime(start_date, date_input_format)\n",
    "        end = datetime.strptime(end_date, date_input_format)\n",
    "\n",
    "        date_range= [start + relativedelta(months=x) for x in range(0, diff_month(end, start)+1)]\n",
    "\n",
    "        self.archive_index = [f\"https://www.nrc.nl/sitemap/{date.strftime(date_nrc_format)}.xml\" for date in date_range]\n",
    "        \n",
    "    def index_urls(self):\n",
    "        \n",
    "        def fetchArchiveURL(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                # Random User Agent\n",
    "                self.headers.update({'User-Agent': self.ua.random,})\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                pass\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_xml'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "\n",
    "            return document\n",
    "        \n",
    "        def fetchArticleURL(url):\n",
    "            doc = fetchArchiveURL(url)\n",
    "            raw_xml = doc['raw_xml']\n",
    "            soup = BeautifulSoup(raw_xml, 'xml')\n",
    "\n",
    "            article_urls = []\n",
    "            locs = soup.find_all('loc')\n",
    "            for loc in locs:\n",
    "                article_urls.append(loc.get_text())\n",
    "\n",
    "            return article_urls\n",
    "        \n",
    "        self.article_urls = []\n",
    "        print('Indexing urls: ')\n",
    "        for url in tqdm(self.archive_index):\n",
    "            # Simulate human usage \n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.article_urls.extend(fetchArticleURL(url))\n",
    "    \n",
    "    def crawl_articles(self):\n",
    "        \n",
    "        def fetchArticle(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                # Random User Agent\n",
    "                self.headers.update({'User-Agent': self.ua.random,})\n",
    "                \n",
    "                r = req.get(url, allow_redirects=False)\n",
    "                r.raise_for_status()\n",
    "\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                pass\n",
    "\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_html'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "                document['parsed'] = 0\n",
    "\n",
    "            return document\n",
    "        \n",
    "        self.articles = []\n",
    "        print('Crawling articles: ')\n",
    "        # Vanaf 36%\n",
    "        # vanaf 44%\n",
    "        for url in tqdm(self.article_urls[15767:]):\n",
    "            # Simulate human usage\n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.articles.append(fetchArticle(url))\n",
    "    \n",
    "    def clean_articles(self):\n",
    "        \n",
    "        def parse_article(r):\n",
    "\n",
    "            a = {}\n",
    "            \n",
    "            try:\n",
    "                a['url']  = r['url']\n",
    "\n",
    "                tree = html.fromstring(r['raw_html'])\n",
    "\n",
    "                try:\n",
    "                    a['timestamp'] = tree.xpath('/html/body/main/div[2]/div/article/div/div/div[1]/div/section/div[1]/div//time[@class=\"article__byline__text\"]/@datetime')[0].strip()\n",
    "                except: \n",
    "                    a['timestamp'] = None\n",
    "                try:\n",
    "                    a['title'] = tree.xpath('//h1/text()')[0].strip()\n",
    "                except:\n",
    "                    a['title'] = None\n",
    "                try:\n",
    "                    a['publisherID'] = tree.xpath('/html/body/main/div[2]/div/article/div/div/div[1]/div/section/div[1]/div/div[1]/ul/li/a/text()')[0].strip()\n",
    "                except:\n",
    "                    a['publisherID'] = None\n",
    "                try:\n",
    "                    a['cleantext'] = ''.join(tree.xpath('//div[@class=\"content article__content\"]//p/text()')[:-3])\n",
    "                except:\n",
    "                    a['cleantext'] = None\n",
    "                try:\n",
    "                    a['category'] = tree.xpath('/html/body/a/div/div/h2/text()')[0].strip()\n",
    "                except:\n",
    "                    a['category'] = None\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            return a\n",
    "        \n",
    "        print('Cleaning articles:')\n",
    "        self.cleaned_articles = []\n",
    "        for article in tqdm(self.articles):\n",
    "            self.cleaned_articles.append(parse_article(article))\n",
    "    \n",
    "    def save_json(self):\n",
    "        \n",
    "        with open('NRC.json', 'w') as file:\n",
    "            json.dump(self.cleaned_articles, file, indent=4)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.index_archive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                               | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing urls: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:27<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "scraper.index_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                            | 0/22567 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling articles: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22567/22567 [11:20:03<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "scraper.crawl_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                 | 68/22567 [00:00<01:07, 334.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning articles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22567/22567 [01:00<00:00, 374.08it/s]\n"
     ]
    }
   ],
   "source": [
    "scraper.clean_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.save_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
