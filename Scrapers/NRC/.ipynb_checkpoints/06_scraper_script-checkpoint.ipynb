{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "\n",
    "import json\n",
    "import w3lib.html\n",
    "\n",
    "from lxml import html\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def index_archive(self):\n",
    "        # Create a list of all monthly sitemap urls\n",
    "        date_input_format = '%m-%Y'\n",
    "        date_nrc_format = '%Y-%m'\n",
    "\n",
    "        # start_date = '10-1970'\n",
    "        # test start date\n",
    "        start_date = '2-2022'\n",
    "        end_date = datetime.today().strftime(date_input_format)\n",
    "\n",
    "        def diff_month(d1, d2):\n",
    "            return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "        start = datetime.strptime(start_date, date_input_format)\n",
    "        end = datetime.strptime(end_date, date_input_format)\n",
    "\n",
    "        date_range= [start + relativedelta(months=x) for x in range(0, diff_month(end, start)+1)]\n",
    "\n",
    "        self.archive_index = [f\"https://www.nrc.nl/sitemap/{date.strftime(date_nrc_format)}.xml\" for date in date_range]\n",
    "        \n",
    "    def index_urls(self):\n",
    "        \n",
    "        def fetchArchiveURL(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                # TODO: add error handling\n",
    "                pass\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_xml'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "\n",
    "            return document\n",
    "        \n",
    "        def fetchArticleURL(url):\n",
    "            doc = fetchArchiveURL(url)\n",
    "            raw_xml = doc['raw_xml']\n",
    "            soup = BeautifulSoup(raw_xml, 'xml')\n",
    "\n",
    "            article_urls = []\n",
    "            locs = soup.find_all('loc')\n",
    "            for loc in locs:\n",
    "                article_urls.append(loc.get_text())\n",
    "\n",
    "            return article_urls\n",
    "        \n",
    "        self.article_urls = []\n",
    "        print('Indexing urls:')\n",
    "        for url in tqdm(self.archive_index):\n",
    "            self.article_urls.extend(fetchArticleURL(url))\n",
    "    \n",
    "    def crawl_articles(self):\n",
    "        \n",
    "        def fetchArticle(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                # TODO: add error handling\n",
    "                pass\n",
    "\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_html'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "                document['parsed'] = 0\n",
    "\n",
    "            return document\n",
    "        \n",
    "        self.articles = []\n",
    "        print('Crawling articles:')\n",
    "        print(self.article_urls[:10])\n",
    "        for url in tqdm(self.article_urls[:10]):\n",
    "            self.articles.append(fetchArticle(url))\n",
    "    \n",
    "    def clean_articles(self):\n",
    "        \n",
    "        def parse_article(r):\n",
    "\n",
    "            a = {}\n",
    "\n",
    "            a['url']  = r['url']\n",
    "\n",
    "            tree = html.fromstring(r['raw_html'])\n",
    "\n",
    "            try:\n",
    "                a['timestamp'] = tree.xpath('/html/body/main/div[2]/div/article/div/div/div[1]/div/section/div[1]/div//time[@class=\"article__byline__text\"]/@datetime')[0].strip()\n",
    "            except: \n",
    "                a['timestamp'] = None\n",
    "            try:\n",
    "                a['title'] = tree.xpath('//h1/text()')[0].strip()\n",
    "            except:\n",
    "                a['title'] = None\n",
    "            try:\n",
    "                a['publisherID'] = tree.xpath('/html/body/main/div[2]/div/article/div/div/div[1]/div/section/div[1]/div/div[1]/ul/li/a/text()')[0].strip()\n",
    "            except:\n",
    "                a['publisherID'] = None\n",
    "            try:\n",
    "                a['cleantext'] = ''.join(tree.xpath('//div[@class=\"content article__content\"]//p/text()')[:-3])\n",
    "            except:\n",
    "                a['cleantext'] = None\n",
    "            try:\n",
    "                a['category'] = tree.xpath('/html/body/a/div/div/h2/text()')[0].strip()\n",
    "            except:\n",
    "                a['category'] = None\n",
    "\n",
    "            return a\n",
    "        \n",
    "        print('Cleaning articles:')\n",
    "        self.cleaned_articles = []\n",
    "        for article in tqdm(self.articles):\n",
    "            self.cleaned_articles.append(parse_article(article))\n",
    "    \n",
    "    def save_json(self):\n",
    "        \n",
    "        with open('NRC.json', 'w') as file:\n",
    "            json.dump(self.articles, file, indent=4)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing urls:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.46it/s]\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling articles:\n",
      "['https://www.nrc.nl/nieuws/2022/02/24/lesje-zoete-aardappel-a4093145', 'https://www.nrc.nl/nieuws/2022/02/24/oekraine-verbreekt-diplomatieke-banden-met-moskou-a4093296', 'https://www.nrc.nl/nieuws/2022/02/24/oekraine-claimt-vijftig-russische-militairen-te-hebben-gedood-a4093295', 'https://www.nrc.nl/nieuws/2022/02/24/loekasjenko-kan-troepen-sturen-indien-nodig-stelt-onderhandelingen-voor-a4093294', 'https://www.nrc.nl/nieuws/2022/02/24/reisadvies-verlaat-oekraine-als-dit-veilig-kan-geen-evacuatie-door-overheid-a4093293', 'https://www.nrc.nl/nieuws/2022/02/24/ik-heb-gehakt-aangevuld-met-tahoe-a4093142', 'https://www.nrc.nl/nieuws/2022/02/24/vrij-a4093159', 'https://www.nrc.nl/nieuws/2022/02/24/en-daarna-toch-maar-weer-zoenen-in-het-bejaardentoilet-a4093158', 'https://www.nrc.nl/nieuws/2022/02/24/niet-te-wild-en-niet-te-duf-a4093154', 'https://www.nrc.nl/nieuws/2022/02/24/kiev-is-in-een-slechte-droom-beland-a4093292']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 250.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning articles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scraper = Scraper()\n",
    "scraper.index_archive()\n",
    "scraper.index_urls()\n",
    "scraper.crawl_articles()\n",
    "scraper.clean_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type bytes is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2cb7f422091b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscraper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-2b9a20294a6f>\u001b[0m in \u001b[0;36msave_json\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NRC.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m# a debuggability cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0m_floatstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Circular reference detected\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type bytes is not JSON serializable"
     ]
    }
   ],
   "source": [
    "scraper.save_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
