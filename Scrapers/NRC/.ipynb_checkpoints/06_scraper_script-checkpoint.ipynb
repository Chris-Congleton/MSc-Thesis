{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "\n",
    "import json\n",
    "import w3lib.html\n",
    "\n",
    "from lxml import html\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def index_archive(self):\n",
    "        # Create a list of all monthly sitemap urls\n",
    "        date_input_format = '%m-%Y'\n",
    "        date_nrc_format = '%Y-%m'\n",
    "\n",
    "        # start_date = '10-1970'\n",
    "        # test start date\n",
    "        start_date = '7-2022'\n",
    "        end_date = datetime.today().strftime(date_input_format)\n",
    "\n",
    "        def diff_month(d1, d2):\n",
    "            return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "        start = datetime.strptime(start_date, date_input_format)\n",
    "        end = datetime.strptime(end_date, date_input_format)\n",
    "\n",
    "        date_range= [start + relativedelta(months=x) for x in range(0, diff_month(end, start)+1)]\n",
    "\n",
    "        self.archive_index = [f\"https://www.nrc.nl/sitemap/{date.strftime(date_nrc_format)}.xml\" for date in date_range]\n",
    "        \n",
    "    def index_urls(self):\n",
    "        \n",
    "        def fetchArchiveURL(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                pass\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_xml'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "\n",
    "            return document\n",
    "        \n",
    "        def fetchArticleURL(url):\n",
    "            doc = fetchArchiveURL(url)\n",
    "            raw_xml = doc['raw_xml']\n",
    "            soup = BeautifulSoup(raw_xml, 'xml')\n",
    "\n",
    "            article_urls = []\n",
    "            locs = soup.find_all('loc')\n",
    "            for loc in locs:\n",
    "                article_urls.append(loc.get_text())\n",
    "\n",
    "            return article_urls\n",
    "        \n",
    "        self.article_urls = []\n",
    "        print('Indexing urls: ')\n",
    "        for url in tqdm(self.archive_index):\n",
    "            # Simulate human usage \n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.article_urls.extend(fetchArticleURL(url))\n",
    "    \n",
    "    def crawl_articles(self):\n",
    "        \n",
    "        def fetchArticle(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                # TODO: add error handling\n",
    "                pass\n",
    "\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_html'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "                document['parsed'] = 0\n",
    "\n",
    "            return document\n",
    "        \n",
    "        self.articles = []\n",
    "        print('Crawling articles: ')\n",
    "        for url in tqdm(self.article_urls):\n",
    "            # Simulate human usage\n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.articles.append(fetchArticle(url))\n",
    "    \n",
    "    def clean_articles(self):\n",
    "        \n",
    "        def parse_article(r):\n",
    "\n",
    "            a = {}\n",
    "\n",
    "            a['url']  = r['url']\n",
    "\n",
    "            tree = html.fromstring(r['raw_html'])\n",
    "\n",
    "            try:\n",
    "                a['timestamp'] = tree.xpath('/html/body/main/div[2]/div/article/div/div/div[1]/div/section/div[1]/div//time[@class=\"article__byline__text\"]/@datetime')[0].strip()\n",
    "            except: \n",
    "                a['timestamp'] = None\n",
    "            try:\n",
    "                a['title'] = tree.xpath('//h1/text()')[0].strip()\n",
    "            except:\n",
    "                a['title'] = None\n",
    "            try:\n",
    "                a['publisherID'] = tree.xpath('/html/body/main/div[2]/div/article/div/div/div[1]/div/section/div[1]/div/div[1]/ul/li/a/text()')[0].strip()\n",
    "            except:\n",
    "                a['publisherID'] = None\n",
    "            try:\n",
    "                a['cleantext'] = ''.join(tree.xpath('//div[@class=\"content article__content\"]//p/text()')[:-3])\n",
    "            except:\n",
    "                a['cleantext'] = None\n",
    "            try:\n",
    "                a['category'] = tree.xpath('/html/body/a/div/div/h2/text()')[0].strip()\n",
    "            except:\n",
    "                a['category'] = None\n",
    "\n",
    "            return a\n",
    "        \n",
    "        print('Cleaning articles:')\n",
    "        self.cleaned_articles = []\n",
    "        for article in tqdm(self.articles):\n",
    "            self.cleaned_articles.append(parse_article(article))\n",
    "    \n",
    "    def save_json(self):\n",
    "        \n",
    "        with open('NRC.json', 'w') as file:\n",
    "            json.dump(self.cleaned_articles, file, indent=4)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing urls: \n",
      "Crawling articles: \n",
      "Cleaning articles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scraper = Scraper()\n",
    "scraper.index_archive()\n",
    "scraper.index_urls()\n",
    "scraper.crawl_articles()\n",
    "scraper.clean_articles()\n",
    "scraper.save_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
