{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "\n",
    "import json\n",
    "import w3lib.html\n",
    "\n",
    "from lxml import html\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def index_archive(self):\n",
    "        date_range = list(pd.date_range('2020-02-01', '2022-03-07', freq='D'))\n",
    "\n",
    "        dates = []\n",
    "        for d in date_range:\n",
    "            dates.append(str(d)[:10].replace('-', '/'))\n",
    "\n",
    "        self.archive_index = [f\"https://www.trouw.nl/archief/{date}\" for date in dates]\n",
    "        \n",
    "    def index_urls(self):\n",
    "        \n",
    "        def fetchArchiveURL(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                pass\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_xml'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "\n",
    "            return document\n",
    "        \n",
    "        def parseArchiveURL(url):\n",
    "            doc = fetchArchiveURL(url)\n",
    "            raw_html = doc['raw_html']\n",
    "            soup = BeautifulSoup(raw_html, 'html')\n",
    "            tree = html.fromstring(raw_html)\n",
    "\n",
    "            try:\n",
    "                article_urls = tree.xpath('/html/body/main/div[2]/*/a[@class=\"teaser__link\"]/@href')\n",
    "            except: \n",
    "                article_urls = None\n",
    "\n",
    "            return article_urls\n",
    "        \n",
    "        self.article_urls = []\n",
    "        print('Indexing urls: ')\n",
    "        for url in tqdm(self.archive_index):\n",
    "            # Simulate human usage \n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.article_urls.extend(parseArchiveURL(url))\n",
    "        \n",
    "        self.article_urls = [\"http://trouw.nl\" + x for x in self.article_urls]\n",
    "    \n",
    "    def crawl_articles(self):\n",
    "        \n",
    "        def fetchArticle(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                # TODO: add error handling\n",
    "                pass\n",
    "\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_html'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "                document['parsed'] = 0\n",
    "\n",
    "            return document\n",
    "        \n",
    "        self.articles = []\n",
    "        print('Crawling articles: ')\n",
    "        for url in tqdm(self.article_urls):\n",
    "            # Simulate human usage\n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.articles.append(fetchArticle(url))\n",
    "    \n",
    "    def clean_articles(self):\n",
    "        \n",
    "        def parse_article(r):\n",
    "            a = {}\n",
    "\n",
    "            a['url']  = r['url']\n",
    "\n",
    "            tree = html.fromstring(r['raw_html'])\n",
    "\n",
    "            try:\n",
    "\n",
    "                a['timestamp'] = tree.xpath('/html/body/main/article/header/section/time[@class=\"artstyle__production__datetime\"]/@datetime')[0].strip()\n",
    "            except: \n",
    "                a['timestamp'] = None\n",
    "            try:\n",
    "                a['title'] = tree.xpath('//h1/text()')[0].strip()\n",
    "            except:\n",
    "                a['title'] = None\n",
    "            try:\n",
    "                a['publisherID'] = tree.xpath('/html/body/main/article/header/section/span[1]/a/text()')[0].strip()\n",
    "            except:\n",
    "                a['publisherID'] = None\n",
    "            try:\n",
    "                a['cleantext'] = ''.join(tree.xpath('//html/body/main/article/section/section/*/text()')[1:])\n",
    "            except:\n",
    "                a['cleantext'] = None\n",
    "            try:\n",
    "                a['category'] = tree.xpath('/html/body/main/section/div[2]/h2/span/a/text()')[0].strip()\n",
    "            except:\n",
    "                a['category'] = None\n",
    "\n",
    "            return a\n",
    "        \n",
    "        print('Cleaning articles:')\n",
    "        self.cleaned_articles = []\n",
    "        for article in tqdm(self.articles):\n",
    "            self.cleaned_articles.append(parse_article(article))\n",
    "    \n",
    "    def save_json(self):\n",
    "        \n",
    "        with open('Trouw.json', 'w') as file:\n",
    "            json.dump(self.cleaned_articles, file, indent=4)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Scraper()\n",
    "scraper.index_archive()\n",
    "scraper.index_urls()\n",
    "scraper.crawl_articles()\n",
    "scraper.clean_articles()\n",
    "scraper.save_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
