{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import json\n",
    "import w3lib.html\n",
    "\n",
    "from lxml import html\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        self.ua = UserAgent()\n",
    "        self.headers = req.utils.default_headers()\n",
    "        self.sitemap_url = 'https://www.telegraaf.nl/sitemap.xml'\n",
    "    \n",
    "    def index_archive(self):\n",
    "        def fetchArchiveURL(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                # Random User Agent\n",
    "                self.headers.update({'User-Agent': self.ua.random,})\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                # TODO: add error handling\n",
    "                pass\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_xml'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "\n",
    "            return document\n",
    "        \n",
    "        def fetchArticleURL(url):\n",
    "            doc = fetchArchiveURL(url)\n",
    "            raw_xml = doc['raw_xml']\n",
    "            soup = BeautifulSoup(raw_xml, 'xml')\n",
    "\n",
    "            article_urls = []\n",
    "            locs = soup.find_all('loc')\n",
    "            for loc in locs:\n",
    "                article_urls.append(loc.get_text())\n",
    "\n",
    "            return article_urls\n",
    "        \n",
    "        self.archive_urls = fetchArticleURL(self.sitemap_url)\n",
    "        \n",
    "    def index_urls(self):\n",
    "        \n",
    "        def fetchArchiveURL(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                # Random User Agent\n",
    "                self.headers.update({'User-Agent': self.ua.random,})\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                # TODO: add error handling\n",
    "                pass\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_xml'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "\n",
    "            return document\n",
    "        \n",
    "        def fetchArticleURL(url):\n",
    "            doc = fetchArchiveURL(url)\n",
    "            raw_xml = doc['raw_xml']\n",
    "            soup = BeautifulSoup(raw_xml, 'xml')\n",
    "\n",
    "            article_urls = []\n",
    "            locs = soup.find_all('loc')\n",
    "            for loc in locs:\n",
    "                article_urls.append(loc.get_text())\n",
    "\n",
    "            return article_urls\n",
    "        \n",
    "        self.article_urls = []\n",
    "        print('Indexing urls: ')\n",
    "        for url in tqdm(self.archive_urls[:5]):\n",
    "            # Simulate human usage \n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.article_urls.extend(fetchArticleURL(url))\n",
    "    \n",
    "    def crawl_articles(self):\n",
    "        def fetchArticle(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                # Random User Agent\n",
    "                self.headers.update({'User-Agent': self.ua.random,})\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                pass\n",
    "\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_html'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "                document['parsed'] = 0\n",
    "\n",
    "            return document\n",
    "        \n",
    "        self.articles = []\n",
    "        print('Crawling articles: ')\n",
    "        for url in tqdm(self.article_urls):\n",
    "            # Simulate human usage\n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.articles.append(fetchArticle(url))\n",
    "    \n",
    "    def clean_articles(self):\n",
    "        \n",
    "        def parse_article(r):\n",
    "            a = {}\n",
    "            \n",
    "            try:\n",
    "                a['url']  = r['url']\n",
    "\n",
    "                tree = html.fromstring(r['raw_html'])\n",
    "\n",
    "                try:\n",
    "                    a['timestamp'] = tree.xpath('/html/body/main/div[2]/div/article/div/div/div[1]/div/section/div[1]/div//time[@class=\"article__byline__text\"]/@datetime')[0].strip()\n",
    "                except: \n",
    "                    a['timestamp'] = None\n",
    "                try:\n",
    "                    a['title'] = tree.xpath('//h1/text()')[0].strip()\n",
    "                except:\n",
    "                    a['title'] = None\n",
    "                try:\n",
    "                    a['publisherID'] = tree.xpath('/html/body/div/article/main/div[1]/section/div[3]/div[1]/div/p/strong/text()')[0].strip()\n",
    "                except:\n",
    "                    a['publisherID'] = None\n",
    "                try:\n",
    "                    a['cleantext'] = tree.xpath('//*[@id=\"articleIntro1179944\"]/text()')[0]\n",
    "                except:\n",
    "                    a['cleantext'] = None\n",
    "                try:\n",
    "                    a['category'] = tree.xpath('/html/body/div/div[1]/a/text()')[0].strip()\n",
    "                except:\n",
    "                    a['category'] = None\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            return a\n",
    "        \n",
    "        print('Cleaning articles:')\n",
    "        self.cleaned_articles = []\n",
    "        for article in tqdm(self.articles):\n",
    "            self.cleaned_articles.append(parse_article(article))\n",
    "    \n",
    "    def save_json(self):\n",
    "        \n",
    "        with open('Telegraaf.json', 'w') as file:\n",
    "            json.dump(self.cleaned_articles, file, indent=4)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.index_archive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing urls: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "scraper.index_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                     | 0/1759 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling articles: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1759/1759 [49:38<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "scraper.crawl_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                          | 25/1759 [00:00<00:07, 247.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning articles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1759/1759 [00:07<00:00, 236.77it/s]\n"
     ]
    }
   ],
   "source": [
    "scraper.clean_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.save_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
