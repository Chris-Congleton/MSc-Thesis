{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import json\n",
    "import w3lib.html\n",
    "\n",
    "from lxml import html\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        self.ua = UserAgent()\n",
    "        self.headers = req.utils.default_headers()\n",
    "    \n",
    "    def index_archive(self): \n",
    "        date_range = list(pd.date_range('2021-02-01', '2022-02-01', freq='D'))\n",
    "\n",
    "        dates = []\n",
    "        for d in date_range:\n",
    "            dates.append(str(d)[:10].replace('-', '/'))\n",
    "\n",
    "        self.archive_index = [f\"https://www.volkskrant.nl/archief/{date}\" for date in dates]\n",
    "        \n",
    "    def index_urls(self):\n",
    "        \n",
    "        def fetchArchiveURL(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                # Random User Agent\n",
    "                self.headers.update({'User-Agent': self.ua.random,})\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                pass\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_html'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "\n",
    "            return document\n",
    "        \n",
    "        def parseArchiveURL(url):\n",
    "            doc = fetchArchiveURL(url)\n",
    "            raw_html = doc['raw_html']\n",
    "            soup = BeautifulSoup(raw_html, 'html')\n",
    "            tree = html.fromstring(raw_html)\n",
    "\n",
    "            try:\n",
    "                article_urls = tree.xpath('/html/body/main/div[2]/*/a[@class=\"teaser__link\"]/@href')\n",
    "            except: \n",
    "                article_urls = None\n",
    "\n",
    "            return article_urls\n",
    "        \n",
    "        self.article_urls = []\n",
    "        print('Indexing urls: ')\n",
    "        for url in tqdm(self.archive_index):\n",
    "            # Simulate human usage \n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.article_urls.extend(parseArchiveURL(url))\n",
    "        \n",
    "        self.article_urls = [\"https://www.volkskrant.nl\" + x for x in self.article_urls]\n",
    "    \n",
    "    def crawl_articles(self):\n",
    "        \n",
    "        def fetchArticle(url):\n",
    "            document = {}\n",
    "\n",
    "            try:\n",
    "                # Random User Agent\n",
    "                self.headers.update({'User-Agent': self.ua.random,})\n",
    "                r = req.get(url)\n",
    "                r.raise_for_status()\n",
    "\n",
    "            except req.exceptions.HTTPError as err:\n",
    "                pass\n",
    "\n",
    "            else:        \n",
    "                document['url'] = url\n",
    "                document['raw_html'] = r.content\n",
    "                document['in_cache_date'] = datetime.utcnow()\n",
    "                document['parsed'] = 0\n",
    "\n",
    "            return document\n",
    "        \n",
    "        self.articles = []\n",
    "        print('Crawling articles: ')\n",
    "        for url in tqdm(self.article_urls):\n",
    "            # Simulate human usage\n",
    "            time.sleep(random.randint(0, 3))\n",
    "            self.articles.append(fetchArticle(url))\n",
    "    \n",
    "    def clean_articles(self):\n",
    "        \n",
    "        def parse_article(r):\n",
    "\n",
    "            a = {}\n",
    "            try:\n",
    "\n",
    "                a['url']  = r['url']\n",
    "\n",
    "                tree = html.fromstring(r['raw_html'])\n",
    "\n",
    "                try:\n",
    "\n",
    "                    a['timestamp'] = tree.xpath('/html/body/main/article/header/section/time[@class=\"artstyle__production__datetime\"]/@datetime')[0].strip()\n",
    "                except: \n",
    "                    a['timestamp'] = None\n",
    "                try:\n",
    "                    a['title'] = tree.xpath('//h1/text()')[0].strip()\n",
    "                except:\n",
    "                    a['title'] = None\n",
    "                try:\n",
    "                    a['publisherID'] = tree.xpath('/html/body/main/article/header/section/span[1]/a/text()')[0].strip()\n",
    "                except:\n",
    "                    a['publisherID'] = None\n",
    "                try:\n",
    "                    a['cleantext'] = ''.join(tree.xpath('//html/body/main/article/section/section/*/text()')[1:])\n",
    "                except:\n",
    "                    a['cleantext'] = None\n",
    "                try:\n",
    "                    a['category'] = tree.xpath('/html/body/main/section/div[2]/h2/span/a/text()')[0].strip()\n",
    "                except:\n",
    "                    a['category'] = None\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            return a\n",
    "        \n",
    "        print('Cleaning articles:')\n",
    "        self.cleaned_articles = []\n",
    "        for article in tqdm(self.articles):\n",
    "            self.cleaned_articles.append(parse_article(article))\n",
    "    \n",
    "    def save_json(self):\n",
    "        \n",
    "        with open('Volkskrant.json', 'w') as file:\n",
    "            json.dump(self.cleaned_articles, file, indent=4)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/366 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing urls: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 366/366 [13:00<00:00,  2.13s/it]\n",
      "  0%|                                                                                        | 0/25773 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling articles: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25773/25773 [14:34:33<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "scraper = Scraper()\n",
    "scraper.index_archive()\n",
    "scraper.index_urls()\n",
    "scraper.crawl_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 27/25773 [00:00<01:44, 246.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning articles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 25773/25773 [01:29<00:00, 287.35it/s]\n"
     ]
    }
   ],
   "source": [
    "scraper.clean_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.save_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
