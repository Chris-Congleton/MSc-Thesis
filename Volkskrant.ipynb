{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Volkskrant.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzUZ8dT0tzqw3TPF8OPJeJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chris-Congleton/MSc-Thesis/blob/main/Volkskrant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1IdcH0Rfllr"
      },
      "source": [
        "# Imports and loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw_Rct6hVjou",
        "outputId": "27a8a94c-c8b8-4707-e96a-32a9498fb6b0"
      },
      "source": [
        "import json, os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt')\n",
        "import seaborn as sns\n",
        "from collections import Counter"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rQ8MfSRTIO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f34c37-b070-4ad0-ca55-d34c542b9e39"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcJ2G3TlgSsd",
        "outputId": "f168367c-770f-4baa-cbf6-8939359600b8"
      },
      "source": [
        "path_to_json = \"gdrive/MyDrive/CS/Thesis/volkskrant-nl\"\n",
        "json_files = [\"gdrive/MyDrive/CS/Thesis/volkskrant-nl/\"+pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
        "print(json_files[:5])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gdrive/MyDrive/CS/Thesis/volkskrant-nl/volkskrant-nl-2002-01.json', 'gdrive/MyDrive/CS/Thesis/volkskrant-nl/volkskrant-nl-2002-02.json', 'gdrive/MyDrive/CS/Thesis/volkskrant-nl/volkskrant-nl-2002-03.json', 'gdrive/MyDrive/CS/Thesis/volkskrant-nl/volkskrant-nl-2002-04.json', 'gdrive/MyDrive/CS/Thesis/volkskrant-nl/volkskrant-nl-2002-05.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxs9IdXmevd8"
      },
      "source": [
        "def readFiles(file_names):\n",
        "  dfs = [] # an empty list to store the data frames\n",
        "  for f in tqdm(file_names):\n",
        "    data = pd.read_json(f) # read data frame from json file\n",
        "    dfs.append(data) # append the data frame to the list\n",
        "\n",
        "  df = pd.concat(dfs, ignore_index=True) # concatenate all the data frames in the list.\n",
        "  return df"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZPOq0K8fzEn",
        "outputId": "224d3b1a-86a4-4f3c-dccc-ded247cadf30"
      },
      "source": [
        "df = readFiles(json_files)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▍    | 117/214 [00:27<00:56,  1.73it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAlGKMOOkZNF"
      },
      "source": [
        "df.tail(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1gu6clfhWa"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mrv8f2CKfLj"
      },
      "source": [
        "f, axs = plt.subplots(1,figsize=(10, 8))\n",
        "sns.histplot(data=df, x=\"categories\", hue=\"categories\", ax=axs)\n",
        "axs.tick_params(axis='x', rotation=45)\n",
        "axs.set_title(\"Total number of articles\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEq9qPiHVy-R"
      },
      "source": [
        "df_year = df.copy(deep=True)\n",
        "df_year[\"year\"] = df_year.timestamp.dt.year\n",
        "time_df = pd.DataFrame({'count':df_year.groupby([\"year\",\"categories\"]).size()}).reset_index()\n",
        "f, axs = plt.subplots(1,figsize=(20, 16))\n",
        "sns.lineplot(data=time_df, x='year',y='count',hue='categories',ci=None, palette=[\"C0\", \"C1\", \"C2\", \"C3\", \"C4\",\n",
        "                                                                                 \"C5\", \"C6\", \"C7\", \"C8\", \"C9\"])\n",
        "axs.set_title(\"Number of articles\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm2mufZJf2ZK"
      },
      "source": [
        "The counts of articles collected per year per category vary quite a bit with for example ~45.000 \"News & Background\" articles in 2012 and ~10.000 in 2019. These should be normalised when counting party or politician mentions for example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3JUGluyWhyP"
      },
      "source": [
        "df_nwords = df.copy(deep=True)\n",
        "df_nwords[\"year\"] = df_nwords.timestamp.dt.year\n",
        "df_nwords[\"nwords\"] = df_nwords.cleantext.str.len()\n",
        "df_nwords = pd.DataFrame(df_nwords.groupby([\"year\",\"categories\"])[\"nwords\"].agg(\"sum\")).reset_index()\n",
        "\n",
        "f, axs = plt.subplots(1,figsize=(20, 16))\n",
        "sns.lineplot(data=df_nwords, x='year',y='nwords',hue='categories',ci=None, palette=[\"C0\", \"C1\", \"C2\", \"C3\", \"C4\",\n",
        "                                                                                 \"C5\", \"C6\", \"C7\", \"C8\", \"C9\"])\n",
        "axs.set_title(\"Total length of articles\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sqAcUvTb8_9"
      },
      "source": [
        "The total amount of words collected is highest for news and background articles. This graph shows a different balance in data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7N93992fgZU"
      },
      "source": [
        "# Frequentie politieke partijen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yThudAzogeBd"
      },
      "source": [
        "# Lijst partijen tweede kamer per jaar: \n",
        "# Bron: https://www.parlement.com/id/vk9scmucgjhj/aantal_deelnemende_partijen_tweede\n",
        "\n",
        "tk_2002 = [\"CDA\", \"LPF\", \"VVD\", \"PvdA\", \"GL\", \"SP\", \"D66\", \"CU\", \"SGP\", \"LN\"]\n",
        "tk_2003 = [\"CDA\", \"LPF\", \"VVD\", \"PvdA\", \"GnL\", \"SP\", \"D66\", \"CU\", \"SGP\"]\n",
        "tk_2006 = [\"CDA\", \"VVD\", \"PvdA\", \"GL\", \"SP\", \"D66\", \"CU\", \"SGP\", \"PVV\", \"PvdD\"]\n",
        "tk_2010 = [\"CDA\", \"VVD\", \"PvdA\", \"GL\", \"SP\", \"D66\", \"CU\", \"SGP\", \"PVV\", \"PvdD\"]\n",
        "tk_2012 = [\"CDA\", \"VVD\", \"PvdA\", \"GL\", \"SP\", \"D66\", \"CU\", \"SGP\", \"PVV\", \"PvdD\", \"50PLUS\"]\n",
        "tk_2017 = [\"CDA\", \"VVD\", \"PvdA\", \"GL\", \"SP\", \"D66\", \"CU\", \"SGP\", \"PVV\", \"PvdD\", \"50PLUS\", \"DENK\", \"FVD\"]\n",
        "tk_2021 = [\"CDA\", \"VVD\", \"PvdA\", \"GL\", \"SP\", \"D66\", \"CU\", \"SGP\", \"PVV\", \"PvdD\", \"50PLUS\", \"DENK\",\n",
        "           \"FVD\", \"BIJ1\", \"BBB\", \"JA21\", \"VOLT\"]\n",
        "\n",
        "# Convert to one unique list\n",
        "tk = [tk_2002, tk_2003,tk_2006,tk_2010,tk_2012,tk_2017,tk_2021]\n",
        "tk = [item for sublist in tk for item in sublist]\n",
        "partijen = list(set(tk))\n",
        "print(partijen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha9eTNODff5H"
      },
      "source": [
        "df_partij = df.copy(deep=True)\n",
        "df_partij[\"year\"] = df_partij.timestamp.dt.year\n",
        "\n",
        "for p in partijen:\n",
        "  df_partij[p] = df_partij.cleantext.str.count(p)\n",
        "\n",
        "df_party_counts = pd.DataFrame()\n",
        "\n",
        "for p in partijen:\n",
        "  df_party_counts[p] = df_partij.groupby([\"year\"])[p].agg(\"sum\")\n",
        "\n",
        "df_party_counts = df_party_counts.reset_index()\n",
        "df_mentions = df_party_counts.melt('year')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_YxrQz_xq3t"
      },
      "source": [
        "df_mentions = df_mentions.rename(columns={'year':'Year','variable':'Party','value':'Frequency'})\n",
        "\n",
        "# Drop rows where frequency is <1000\n",
        "df_mentions = df_mentions[df_mentions.Frequency > 1000]\n",
        "print(df_mentions.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcajUCOIoHVO"
      },
      "source": [
        "f, axs = plt.subplots(1,figsize=(20, 16))\n",
        "sns.lineplot(data=df_mentions, x='Year',y='Frequency', hue='Party')\n",
        "axs.set_title(\"Party mentions in Volkskrant\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF-1mjQWccom"
      },
      "source": [
        "# Frequentie politici"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvDCNRG4ce49"
      },
      "source": [
        "politici = [\"Kees van der Staaij\", \"Sigrid Kaag\", \"Lilian Marijnissen\", \"Geert Wilders\", \"Wopke Hoekstra\", \"Lilianne Ploumen\", \"Mark Rutte\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDs_baZoc7_5"
      },
      "source": [
        "# Computing number of articles per year to normalise counts\n",
        "df_norm = time_df.groupby(\"year\").sum(\"count\").reset_index()\n",
        "df_norm = df_norm.rename(columns={\"year\":\"Year\", \"count\":\"Articles\"})\n",
        "print(df_norm.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJWTvOp7c_7U"
      },
      "source": [
        "# Count occurence of politicians\n",
        "df_politici = df.copy(deep=True)\n",
        "df_politici[\"year\"] = df_politici.timestamp.dt.year\n",
        "\n",
        "for p in politici:\n",
        "  df_politici[p] = df_politici.cleantext.str.count(p)\n",
        "\n",
        "df_politici_counts = pd.DataFrame()\n",
        "\n",
        "for p in politici:\n",
        "  df_politici_counts[p] = df_politici.groupby([\"year\"])[p].agg(\"sum\")\n",
        "\n",
        "df_politici_counts = df_politici_counts.reset_index()\n",
        "df_pol_mentions = df_politici_counts.melt('year')\n",
        "\n",
        "df_pol_mentions = df_pol_mentions.rename(columns={'year':'Year','variable':'Politician','value':'Frequency'})\n",
        "\n",
        "df_pol_mentions.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87zit3YsfZDt"
      },
      "source": [
        "df_pol_mentions = df_pol_mentions.merge(df_norm, on=\"Year\")\n",
        "print(df_pol_mentions.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxgrqJFHfL2N"
      },
      "source": [
        "# Drop rows where frequency is <1000\n",
        "# df_pol_mentions = df_pol_mentions[df_pol_mentions.Frequency > 1000]\n",
        "# print(df_pol_mentions.head())\n",
        "df_pol_mentions[\"Normalised Freq\"] = df_pol_mentions[\"Frequency\"]/df_pol_mentions[\"Articles\"]*10000\n",
        "print(df_pol_mentions.head())\n",
        "\n",
        "f, axs = plt.subplots(1,figsize=(20, 16))\n",
        "sns.lineplot(data=df_pol_mentions, x='Year',y='Normalised Freq', hue='Politician')\n",
        "axs.set_title(\"Politician mentions in Volkskrant (normalised)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr6ioGRcgif1"
      },
      "source": [
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from gensim import utils\n",
        "import gensim.models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_knFv-tWS89"
      },
      "source": [
        "volkskrant_corpus = list(df[\"cleantext\"].values[1000])\n",
        "print(len(volkskrant_corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yAyqctTXs8s"
      },
      "source": [
        "class VolkskrantCorpus():\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __init__(self, corpus):\n",
        "      self.corpus = corpus\n",
        "\n",
        "    def __iter__(self):\n",
        "        for line in self.corpus:\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wra_t2_cZoVG"
      },
      "source": [
        "sentences = VolkskrantCorpus(volkskrant_corpus)\n",
        "model = gensim.models.Word2Vec(sentences=sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hskkuPcdaglT"
      },
      "source": [
        "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
        "from sklearn.manifold import TSNE                   # final reduction\n",
        "import numpy as np                                  # array handling\n",
        "\n",
        "\n",
        "def reduce_dimensions(model):\n",
        "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
        "\n",
        "    # extract the words & their vectors, as numpy arrays\n",
        "    vectors = np.asarray(model.wv.vectors)\n",
        "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
        "\n",
        "    # reduce using t-SNE\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    x_vals = [v[0] for v in vectors]\n",
        "    y_vals = [v[1] for v in vectors]\n",
        "    return x_vals, y_vals, labels\n",
        "\n",
        "\n",
        "x_vals, y_vals, labels = reduce_dimensions(model)\n",
        "\n",
        "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
        "    from plotly.offline import init_notebook_mode, iplot, plot\n",
        "    import plotly.graph_objs as go\n",
        "\n",
        "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
        "    data = [trace]\n",
        "\n",
        "    if plot_in_notebook:\n",
        "        init_notebook_mode(connected=True)\n",
        "        iplot(data, filename='word-embedding-plot')\n",
        "    else:\n",
        "        plot(data, filename='word-embedding-plot.html')\n",
        "\n",
        "\n",
        "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import random\n",
        "\n",
        "    random.seed(0)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.scatter(x_vals, y_vals)\n",
        "\n",
        "    #\n",
        "    # Label randomly subsampled 25 data points\n",
        "    #\n",
        "    indices = list(range(len(labels)))\n",
        "    selected_indices = random.sample(indices, 25)\n",
        "    for i in selected_indices:\n",
        "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
        "\n",
        "try:\n",
        "    get_ipython()\n",
        "except Exception:\n",
        "    plot_function = plot_with_matplotlib\n",
        "else:\n",
        "    plot_function = plot_with_plotly\n",
        "\n",
        "plot_function(x_vals, y_vals, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJbaEKSYbnLF"
      },
      "source": [
        "import tempfile\n",
        "\n",
        "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
        "    temporary_filepath = tmp.name\n",
        "    model.save(temporary_filepath)\n",
        "    # The model is now safely stored in the filepath.\n",
        "    # You can copy it to other machines, share it with others, etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHYDlDhqcWog"
      },
      "source": [
        "# To load a saved model:\n",
        "new_model = gensim.models.Word2Vec.load(temporary_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}